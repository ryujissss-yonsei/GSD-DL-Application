{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada652f2",
   "metadata": {},
   "source": [
    "# Libirary import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b118d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" os \"\"\"\n",
    "import os\n",
    "\n",
    "\"\"\" torch \"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import Tensor\n",
    "\n",
    "\"\"\"glob\"\"\"\n",
    "from glob import glob\n",
    "\n",
    "\"\"\" tqdm \"\"\"\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"Pandas\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" numpy \"\"\"\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"JSON\"\"\"\n",
    "import json\n",
    "\n",
    "\"\"\"sklearn\"\"\"\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "\"\"\"seaborn\"\"\"\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"scipy\"\"\"\n",
    "from scipy import io\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, ifft,fftfreq\n",
    "from scipy import stats\n",
    "\n",
    "\"\"\"time\"\"\"\n",
    "import time\n",
    "\n",
    "\"\"\"PIL\"\"\"\n",
    "from PIL import Image\n",
    "\n",
    "import re\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import imageio\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24acefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee31610c",
   "metadata": {},
   "source": [
    "# Path init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d75336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "sys.path.append('/Function')\n",
    "from preprocessing import get_x_y_pairs, preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a538ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "work_path = \"\" # Our data paths, it is not opened\n",
    "os.chdir(work_path)\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1215e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40417641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23f117a2",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    #init\n",
    "    \"Task\"              :\"Time series forecasting\",\n",
    "    \"Dataset\"           :\"GSD patients data\", \n",
    "    \"Patient\"           :'more then 14 days patients',\n",
    "    \"Data_Balanced\"     :\"Unknown\",\n",
    "    \"Filtering\"         :\"None\",\n",
    "    \"Normalization\"     :\"RobustScaler\",\n",
    "    \"Loss\"              :\"mse loss\",\n",
    "    \"Preprocess\"        :\"\",\n",
    "    \"Model_name\"        :\"TSMixer\",\n",
    "    \"basic_path\"        :\"\", # result save path\n",
    "    \n",
    "    #hyper parameters\n",
    "    \"seed\"              :1,\n",
    "    \"lr\"                :0.005,\n",
    "    \"batch_size\"        :128,  # 4096\n",
    "    \n",
    "    \"test_batch_size\"   :4096,\n",
    "    \"window_size\"       :48, # 12 hours\n",
    "    \"forcast_size\"      :4, # 1 hour\n",
    "    \"epochs\"            :100,\n",
    "    \"no_cuda\"           :False,\n",
    "    \"log_interval\"      :100,\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183060f",
   "metadata": {},
   "source": [
    "# Set the seed and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47513d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "kwargs = {'num_workers':0,'pin_memory':True} if use_cuda else {}\n",
    "\n",
    "!nvcc --version\n",
    "#https://pytorch.org/get-started/previous-versions/\n",
    "print('--------------------------------------')\n",
    "print('현재 torch 버전:',torch.__version__)\n",
    "print('학습을 진행하는 기기:',device)\n",
    "print('cuda index:', torch.cuda.current_device())\n",
    "print('gpu 개수:', torch.cuda.device_count())\n",
    "print('graphic name:', torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f49cf",
   "metadata": {},
   "source": [
    "# Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Model')\n",
    "from N_Linear import LTSF_NLinear\n",
    "from D_Linear import LTSF_DLinear\n",
    "from TSMixer import TSMixer\n",
    "from PatchTST import PatchTST\n",
    "\n",
    "from Model_performance import model_performance_forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d7f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fcc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4a35957",
   "metadata": {},
   "source": [
    "# 1. TS Mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76facda",
   "metadata": {},
   "outputs": [],
   "source": [
    "input  = torch.randn(256, args.window_size, 7) \n",
    "target = torch.randn(256, args.forcast_size, 7) \n",
    "\n",
    "model = TSMixer(sequence_length=args.window_size, prediction_length=args.forcast_size, input_channels=7, output_channels=7, num_blocks=8)\n",
    "\n",
    "output = model(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736de72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab3ec1d6",
   "metadata": {},
   "source": [
    "# 2. MLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86d1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input  = torch.randn(256, args.window_size, 7) \n",
    "target = torch.randn(256, args.forcast_size, 7) \n",
    "\n",
    "\n",
    "model = LTSF_NLinear(window_size=args.window_size, forcast_size=args.forcast_size, individual=False, feature_size=7)\n",
    "\n",
    "\n",
    "\n",
    "output = model(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4173093c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2568ab68",
   "metadata": {},
   "source": [
    "# 3.PatchTST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "input  = torch.randn(256, args.window_size, 7) \n",
    "target = torch.randn(256, args.forcast_size, 7) \n",
    "\n",
    "model = PatchTST(channel = 7, window_size=args.window_size, forcast_size=args.forcast_size)\n",
    "\n",
    "\n",
    "\n",
    "output = model(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd4159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b102f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc63267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8963ee6",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb76a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_indexs, data_df, transform=None):\n",
    "        self.data_indexs = data_indexs\n",
    "        self.data_df     = data_df\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        start = self.data_indexs[idx]\n",
    "        X  = self.data_df[start:start+args.window_size] # window\n",
    "        Y  = self.data_df[start+args.window_size:start+args.window_size+args.forcast_size] # forecast\n",
    "        \n",
    "        \n",
    "        \"\"\"data\"\"\"\n",
    "        year      = np.expand_dims(np.array(X['Year']),axis=1)\n",
    "        month     = np.expand_dims(np.array(X['Month']),axis=1)\n",
    "        day       = np.expand_dims(np.array(X['Day']),axis=1)\n",
    "        minutes   = np.expand_dims(np.array(X['Minutes']),axis=1)\n",
    "        min_sin   = np.expand_dims(np.array(X['Minutes_sin']),axis=1)\n",
    "        min_cos   = np.expand_dims(np.array(X['Minutes_cos']),axis=1)\n",
    "        glu       = np.expand_dims(np.array(X['Glucose_level_original']),axis=1)\n",
    "        \n",
    "\n",
    "        data = np.concatenate((year, month, day, minutes, min_sin, min_cos, glu),axis=1)\n",
    "        \n",
    "        \n",
    "        \"\"\"get label\"\"\"\n",
    "        \n",
    "        year      = np.expand_dims(np.array(Y['Year']),axis=1)\n",
    "        month     = np.expand_dims(np.array(Y['Month']),axis=1)\n",
    "        day       = np.expand_dims(np.array(Y['Day']),axis=1)\n",
    "        minutes   = np.expand_dims(np.array(Y['Minutes']),axis=1)\n",
    "        min_sin   = np.expand_dims(np.array(Y['Minutes_sin']),axis=1)\n",
    "        min_cos   = np.expand_dims(np.array(Y['Minutes_cos']),axis=1)\n",
    "        glu       = np.expand_dims(np.array(Y['Glucose_level_original']),axis=1)\n",
    "        \n",
    "\n",
    "        label = np.concatenate((year, month, day, minutes, min_sin, min_cos, glu),axis=1) # B, 4, 7\n",
    "\n",
    "        \n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_indexs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50800a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7488cad4",
   "metadata": {},
   "source": [
    "# Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96671e9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_paths = os.listdir()\n",
    "data_paths\n",
    "len(data_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e132e5",
   "metadata": {},
   "source": [
    "# Patients selection(more than 14 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_paths = []\n",
    "\n",
    "for i in range(0,len(data_paths),1):\n",
    "    data_path = data_paths[i]\n",
    "    df = pd.read_csv(data_path, engine='python', encoding='utf-8')\n",
    "    \n",
    "    day=np.round(len(df)/(4*24), 1)\n",
    "    if day < 14:\n",
    "        pass\n",
    "    else:\n",
    "        new_data_paths.append(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6013184",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_data_paths)\n",
    "data_paths = new_data_paths\n",
    "len(data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c62d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3364965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d38363c7",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1200fd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(0,len(data_paths),1):\n",
    "    \n",
    "    \n",
    "    \"\"\"init\"\"\"\n",
    "    data_path           = data_paths[k]\n",
    "    args.experiment_num = k\n",
    "    df                  = pd.read_csv(data_path)\n",
    "    \n",
    "    \"\"\"make folder\"\"\"\n",
    "    result_save_path = args.basic_path + args.Model_name +'/'+str(args.experiment_num)\n",
    "\n",
    "    folder_path = result_save_path\n",
    "    try:\n",
    "        if not(os.path.isdir(folder_path)):\n",
    "            os.makedirs(os.path.join(folder_path))\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            print(\"Failed to create directory!!!!!\")\n",
    "            raise\n",
    "            \n",
    "            \n",
    "    \"\"\"---------------------------------------------Dataset split---------------------------------------------\"\"\"\n",
    "    train_df, valid_df, test_df = preprocessing(df, RobustScaler())\n",
    "    print(len(train_df), len(valid_df), len(test_df))\n",
    "    \n",
    "    \"\"\"---------------------------------------------Make pair---------------------------------------------\"\"\"\n",
    "    train_indexs   = get_x_y_pairs(train_df, args.window_size, args.forcast_size)\n",
    "    valid_indexs   = get_x_y_pairs(valid_df, args.window_size, args.forcast_size)\n",
    "    test_indexs  = get_x_y_pairs(test_df, args.window_size, args.forcast_size) # 전체 시간대\n",
    "\n",
    "    print(len(train_indexs),len(valid_indexs),len(test_indexs))\n",
    "    \n",
    "    \"\"\"---------------------------------------------Custom dataset---------------------------------------------\"\"\"\n",
    "    train_dataset      = CustomDataset(train_indexs, train_df, transforms.Compose([transforms.ToTensor()]))\n",
    "    validation_dataset = CustomDataset(valid_indexs, valid_df, transforms.Compose([transforms.ToTensor()]))\n",
    "    test_dataset       = CustomDataset(test_indexs, test_df, transforms.Compose([transforms.ToTensor()])) # 전체 시간대\n",
    "\n",
    "    print(len(train_dataset),len(validation_dataset), len(test_dataset))\n",
    "    \n",
    "    \"\"\"---------------------------------------------Data Loader---------------------------------------------\"\"\"\n",
    "    \"\"\"Train\"\"\"\n",
    "    args.train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size = args.batch_size,\n",
    "        shuffle = False, \n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    \"\"\"Validation\"\"\"\n",
    "    args.validation_loader = torch.utils.data.DataLoader(\n",
    "        dataset=validation_dataset,\n",
    "        batch_size = args.batch_size,\n",
    "        shuffle = False,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    \"\"\"Test\"\"\"\n",
    "    args.test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size = args.test_batch_size,\n",
    "        shuffle = False,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    print(\"Length of the train_loader:\", len(args.train_loader))\n",
    "    print(\"Length of the val_loader:\", len(args.validation_loader))\n",
    "    print(\"Length of the test_loader:\", len(args.test_loader))\n",
    "    \n",
    "    \"\"\"---------------------------------------------optimizer---------------------------------------------\"\"\"\n",
    "    args.device = device\n",
    "    \n",
    "    #args.model = TSMixer(sequence_length=args.window_size, prediction_length=args.forcast_size, input_channels=7, output_channels=7, num_blocks=8).to(device)\n",
    "    #args.model = LTSF_NLinear(window_size=args.window_size, forcast_size=args.forcast_size, individual=False, feature_size=7).to(device)\n",
    "    args.model= PatchTST(channel = 7, window_size=args.window_size, forcast_size=args.forcast_size).to(device)\n",
    "    \n",
    "    \n",
    "    args.optimizer = optim.Adam(args.model.parameters(), lr=args.lr)\n",
    "    args.criterion = nn.MSELoss()\n",
    "    \n",
    "    \n",
    "    \"\"\"---------------------------------------------Train---------------------------------------------\"\"\"\n",
    "    path = folder_path\n",
    "    \n",
    "    train_loader      = args.train_loader\n",
    "    validation_loader = args.validation_loader\n",
    "\n",
    "    model     = args.model\n",
    "    optimizer = args.optimizer\n",
    "    criterion = args.criterion\n",
    "\n",
    "    device = args.device\n",
    "\n",
    "    train_losses        = []\n",
    "    avg_train_losses    = []\n",
    "    Train_baths_ACC     = [] \n",
    "    Train_ACC           = [] \n",
    "    Train_AUROC         = []\n",
    "\n",
    "\n",
    "    \"\"\"Validaion\"\"\"\n",
    "    valid_losses        = []\n",
    "    avg_valid_losses    = []\n",
    "    Validation_ACC      = []\n",
    "    Valid_ACC_per_Class = []\n",
    "    Validation_AUROC    = []\n",
    "\n",
    "    best_loss  = 100000000000\n",
    "    #best_MAE   = 100000\n",
    "    #best_MSE   = 100000\n",
    "    best_MAPE  = 100000000000\n",
    "    best_epoch = 0\n",
    "    best_model_save_path = path +'/'+ 'best model of experiment ' + str(args.experiment_num)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "\n",
    "        \"\"\"Train\"\"\"\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch_idx, (data,target) in enumerate(train_loader):\n",
    "            data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data).to(device)\n",
    "\n",
    "            \"\"\"loss\"\"\"\n",
    "            loss = 0\n",
    "            for i in range(output.shape[-1]):\n",
    "                pred   = output[:,:,i]\n",
    "                actual = target[:,:,i]\n",
    "                loss += criterion(pred.view_as(actual), actual)\n",
    "\n",
    "            \"\"\"update and save loss\"\"\"\n",
    "            train_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "\n",
    "                #1.\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "                \n",
    "        \"\"\"Validation\"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        valid_loss = 0\n",
    "        total = len(validation_loader.dataset)\n",
    "\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "                data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "\n",
    "                output = model(data).to(device)\n",
    "                \n",
    "                \"\"\"loss\"\"\"\n",
    "                loss = 0\n",
    "                for i in range(output.shape[-1]):\n",
    "                    pred   = output[:,:,i]\n",
    "                    actual = target[:,:,i]\n",
    "                    loss += criterion(pred.view_as(actual), actual)\n",
    "                valid_loss += loss\n",
    "                \n",
    "                true_labels.append(target.detach().cpu().numpy()) \n",
    "                pred_labels.append(output.detach().cpu().numpy())\n",
    "\n",
    "                \n",
    "\n",
    "            \"\"\"caclulate performance\"\"\"\n",
    "            valid_df = model_performance_forecasting(pred_labels, true_labels)\n",
    "\n",
    "\n",
    "            \"\"\"Loss and ACC \"\"\"\n",
    "            train_loss /= len(train_loader)\n",
    "            valid_loss /= len(validation_loader)\n",
    "            avg_train_losses.append(train_loss.cpu().numpy())\n",
    "            avg_valid_losses.append(valid_loss.cpu().numpy())\n",
    "\n",
    "            print('------------------------------------------')\n",
    "            print('Valid set: Average loss: {:.4f}'.format(valid_loss))\n",
    "            print('------------------------------------------')\n",
    "            print('Valid set: Year MAPE: {:.4f}'.format(valid_df[valid_df['Varitare'] == 'year']['MAPE'].values[0]))\n",
    "            print('Valid set: Month MAPE: {:.4f}'.format(valid_df[valid_df['Varitare'] == 'month']['MAPE'].values[0]))\n",
    "            print('Valid set: Day MAPE: {:.4f}'.format(valid_df[valid_df['Varitare'] == 'day']['MAPE'].values[0]))\n",
    "            print('Valid set: Minutes MAPE: {:.4f}'.format(valid_df[valid_df['Varitare'] == 'minutes']['MAPE'].values[0]))\n",
    "            print('Valid set: Min_sin MAPE: {:.4f}'.format(valid_df[valid_df['Varitare'] == 'min_sin']['MAPE'].values[0]))\n",
    "            print('Valid set: Min_cos MAPE: {:.4f}'.format(valid_df[valid_df['Varitare'] == 'min_cos']['MAPE'].values[0]))\n",
    "            print('-------------------------------------------')\n",
    "            print('Valid set: Glucose MAE: {:.4f}'.format(valid_df[valid_df['Varitare'] == 'glu']['MAE'].values[0]))\n",
    "            print('Valid set: Glucose MSE: {:.4f}'.format(valid_df[valid_df['Varitare'] == 'glu']['MSE'].values[0]))\n",
    "            print('Valid set: Glucose MAPE: {:.4f}'.format(valid_df[valid_df['Varitare'] == 'glu']['MAPE'].values[0]))\n",
    "            print('-------------------------------------------')\n",
    "\n",
    "            \"\"\"Save best model\"\"\"\n",
    "            if valid_df[valid_df['Varitare'] == 'glu']['MAPE'].values[0] < best_MAPE:\n",
    "                torch.save(model, best_model_save_path)\n",
    "                print(\"best model was saved.\")\n",
    "                print('-------------------------------------------')\n",
    "                best_loss = valid_loss\n",
    "                best_epoch = epoch\n",
    "                best_MAPE = valid_df[valid_df['Varitare'] == 'glu']['MAPE'].values[0]\n",
    "                #best_MAE   = MAE\n",
    "                #best_MSE   = MSE\n",
    "                #best_MAPE  = MAPE\n",
    "                \n",
    "                valid_df['best_epoch'] = best_epoch\n",
    "                valid_df.to_excel(folder_path +'/Valid Result.xlsx',index=True)\n",
    "                \n",
    "            print('----------------------------------------------------------------')\n",
    "\n",
    "    \n",
    "    \"\"\"figure save\"\"\"\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    plt.plot(range(1,len(avg_train_losses)+1),avg_train_losses, label='Training Loss')\n",
    "    plt.plot(range(1,len(avg_valid_losses)+1),avg_valid_losses, label='Validation Loss')\n",
    "\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.xlim(0, len(avg_train_losses)+1) \n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path +'/Loss.png', dpi = 300) \n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n",
    "    \"\"\"---------------------------------------------Test---------------------------------------------\"\"\"\n",
    "    test_loader  = args.test_loader\n",
    "    \n",
    "    device = args.device\n",
    "    criterion = args.criterion\n",
    "\n",
    "    best_model_save_path = path +'/'+ 'best model of experiment ' + str(args.experiment_num)\n",
    "    model = torch.load(best_model_save_path)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "\n",
    "            data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "\n",
    "            output = model(data).to(device)\n",
    "\n",
    "            \"\"\"loss\"\"\"\n",
    "            loss = 0\n",
    "            for i in range(output.shape[-1]):\n",
    "                pred   = output[:,:,i]\n",
    "                actual = target[:,:,i]\n",
    "                loss += criterion(pred.view_as(actual), actual)\n",
    "            test_loss += loss\n",
    "\n",
    "\n",
    "            true_labels.append(target.detach().cpu().numpy()) \n",
    "            pred_labels.append(output.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        print('Test set: Average loss(MSE): {:.4f}'.format(test_loss))\n",
    "        test_df = model_performance_forecasting(pred_labels, true_labels)\n",
    "        test_df.to_excel(folder_path +'/Test Result.xlsx',index=True)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    \"\"\"---------------------------------------------Save---------------------------------------------\"\"\"\n",
    "    args_copy = args.copy()\n",
    "    del_list = ['train_loader','validation_loader','test_loader','model']\n",
    "\n",
    "    for i in range(len(del_list)):\n",
    "        del args_copy[del_list[i]]\n",
    "\n",
    "    df = pd.DataFrame(args_copy,index = [0]).T\n",
    "    df.to_excel(folder_path +'/Settings.xlsx',index=True)\n",
    "\n",
    "    print('-------------------------------[{}/{}]-------------------------------'.format(k, len(data_paths)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
